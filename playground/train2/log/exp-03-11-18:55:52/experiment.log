2019-03-11 18:55:52,952-Start of experiment
2019-03-11 18:55:52,952-=========== Initilized logger =============
2019-03-11 18:55:52,953-
	arch: simple29_encoderdecoder
	batch_size: 1
	ckpt: None
	dataset: cityscape
	disp_interval: 10
	embedding_dim: 15
	epochs: 20
	lr: 0.001
	lr_decay_gamma: 0.1
	lr_decay_step: 5
	optimizer: adam
	path: ../log/exp-03-11-18:55:52
	port: 54257
	print_freq: 10
	seed: 1024
	start_epoch: 1
	test_dir: /data/agong/test
	train_dir: /data/agong/train
	val: False
	val_dir: /data/agong/val
	val_interval: 1
	workers: 4
2019-03-11 18:55:53,513-Total number of gpus: 1
2019-03-11 18:55:54,177 - [worker 0] - Initializing trainer
2019-03-11 18:55:58,509 - [worker 0] - Model info
DistributedDataParallel(
  (module): Simple(
    (embedding): Embedding(30, 15)
    (layer): EncoderDecoder(
      (encoder): Sequential(
        (0): Conv2d(15, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
        (1): ReLU()
        (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (3): ReLU()
        (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (5): ReLU()
        (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (7): ReLU()
        (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (9): ReLU()
        (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (11): ReLU()
      )
      (dilated_layer): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2))
        (1): ReLU()
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4))
        (3): ReLU()
        (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(8, 8), dilation=(8, 8))
        (5): ReLU()
        (6): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(16, 16), dilation=(16, 16))
        (7): ReLU()
      )
      (bottle_neck): Sequential(
        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU()
        (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (3): ReLU()
      )
      (decoder): Sequential(
        (0): Upsample(scale_factor=2, mode=bilinear)
        (1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): ReLU()
        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (4): ReLU()
        (5): Upsample(scale_factor=2, mode=bilinear)
        (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (7): ReLU()
        (8): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (9): ReLU()
        (10): Conv2d(64, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
  )
)
2019-03-11 18:55:58,539 - [worker 0] - Start of epoch 1
2019-03-11 18:55:58,539 - [worker 0] - Training started
2019-03-11 18:56:01,666 - [worker 0] - torch.Size([1, 1024, 2048])
2019-03-11 18:56:01,666 - [worker 0] - torch.Size([1, 1024, 2048])
2019-03-11 18:56:01,666 - [worker 0] - torch.Size([1, 29])
2019-03-11 18:56:05,353 - [worker 0] - tensor(0.4702, device='cuda:0', grad_fn=<DivBackward0>)
2019-03-11 18:56:11,958 - [worker 0] - Epoch [1/20][1/2975] load [3.125s] comp [9.062s] loss [0.4702]
2019-03-11 18:56:11,959 - [worker 0] - torch.Size([1, 1024, 2048])
2019-03-11 18:56:11,960 - [worker 0] - torch.Size([1, 1024, 2048])
2019-03-11 18:56:11,960 - [worker 0] - torch.Size([1, 29])
2019-03-11 18:56:12,351 - [worker 0] - tensor(0.4903, device='cuda:0', grad_fn=<DivBackward0>)
2019-03-11 18:56:12,955 - [worker 0] - torch.Size([1, 1024, 2048])
2019-03-11 18:56:12,955 - [worker 0] - torch.Size([1, 1024, 2048])
2019-03-11 18:56:12,955 - [worker 0] - torch.Size([1, 29])
2019-03-11 18:56:14,788 - [worker 0] - tensor(0.4925, device='cuda:0', grad_fn=<DivBackward0>)
2019-03-11 18:56:15,301 - [worker 0] - torch.Size([1, 1024, 2048])
2019-03-11 18:56:15,301 - [worker 0] - torch.Size([1, 1024, 2048])
2019-03-11 18:56:15,301 - [worker 0] - torch.Size([1, 29])
2019-03-11 18:56:16,883 - [worker 0] - tensor(0.4668, device='cuda:0', grad_fn=<DivBackward0>)
